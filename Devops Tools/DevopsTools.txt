DAY1: Containerization (Docker):

Docker:it is isolated environment with libraries ,application runtime and dependencies.It is lightweight sandbox environment.Everything that is required to run an application like (Build->ship->run)

Understanding Containers V/S Virtual Machines:
Containers:The underlying hardware is physical server,it has only one OS runs on the host machines ,container engine like software like docker or kubernets that manages containers.each container include only the application and its dependencies but share the host OS kernels .
Virtual Machines:It has physical hardware like server storage networking.The OS installed on the physical machines eg window,linux.software like VMware ,virtual box that creates and manages multiple VMs and in guest OS ech vm runs its owns operating system which increases overhead ,and each VM has its own library binaries and dependencies to run application

Containers V/S Virtual machines with the help of a Building and House analogy:
Containers(Building):it is like shared but infrastructure isolated system 
Virtual Machines(House):it is like operating system isolated infrastructure.

A Simple Docker WorkFlow:
It starts with a writing a docker file ,which defines the application environment,including the base OS,dependencies and commands to run the application.next you build docker image using docker build,which packages everything into a reusable format then you run a container from this image using docker run which launches the application in an isolated environment.if needed you can push the image to docker hub for sharing or deployment.finally to manage multiple containers you can use docker compose or kubernets for scalling and automations.

Docker Architecture:
It follows a client server Architecture,the docker client interacts with the docker daemon,which manages container,images networks and storage .the docker daemon runs in the background on the host machine and listens for the command from the client.the docker engine is the core componenet that include daemon,CLI,and API.images are light weight standalone packages contaning application code dependencies and configurations.when an image runs it creates a container an isolated runtime environment.docker also uses registries like docker hub to store share images .this Architecture makes application portable scalable and efficient.

DAY2:How To Dockerize a Project 
Get started
Clone a sample git repository using the below command or use your project for the demo:
cd into the directory
Create an empty file with the name Dockerfile=touch Dockerfile
Using the text editor of your choice, paste the below content ( Details about each of these have already shared in the video):
FROM node:18-alpine
WORKDIR /app
COPY . .
RUN yarn install --production
CMD ["node", "src/index.js"]
EXPOSE 3000

Build the docker image using the application code and Dockerfile=docker build -t day02-todo .

Verify the image has been created and stored locally using the below command:docker images

Create a public repository on hub.docker.com and push the image to remote repo:
docker login
docker tag day02-todo:latest username/new-reponame:tagname
docker images
docker push username/new-reponame:tagname

To pull the image to another environment , you can use below command
docker pull username/new-reponame:tagname

To start the docker container, use below command
docker run -dp 3000:3000 username/new-reponame:tagname

Verify your app. If you have followed the above steps correctly, your app should be listening on localhost:3000
To enter(exec) into the container, use the below command
docker exec -it containername sh
or
docker exec -it containerid sh

To view docker logs
docker logs containername
or
docker logs containerid



DAY3:
Getting started with the demo:Clone the below sample repository, or you can use any web application that you have=git clone https://github.com/piyushsachdeva/todoapp-docker.git
cd into the directory
cd todoapp-docker/
Create an empty file with the name Dockerfile=touch Dockerfile
Using the text editor of your choice, paste the below content: Note: Details about the below Dockerfile have already been shared in the video=FROM node:18-alpine AS installer
WORKDIR /app
COPY package*.json ./
RUN npm install 
COPY . .
RUN npm run build
FROM nginx:latest AS deployer
COPY --from=installer /app/build /usr/share/nginx/html
Build the docker image using the application code and Dockerfile
docker build -t todoapp-docker .
Verify the image has been created and stored locally using the below command:
docker images
Create a public repository on hub.docker.com and push the image to remote repo
docker login
docker tag todoapp-docker:latest username/new-reponame:tagname
docker images
docker push username/new-reponame:tagname
To pull the image to another environment, you can use the below command
docker pull username/new-reponame:tagname
To start the docker container, use the below command
docker run -dp 3000:80 username/new-reponame:tagname
Verify your app. If you have followed the above steps correctly, your app should be listening on localhost:3000
To enter(exec) into the container, use the below command
docker exec -it containername sh
or
docker exec -it containerid sh
To view docker logs
docker logs containername
or
docker logs containerid
To view the content of Docker container
docker inspect
Cleanup the old docker images from local repo using below command:
docker image rm image-id


DAY4:
Challenges of Using Standalone Containers:
1.Manual scaling-you have to manually start,stop,scale containers based on demand.
2.Networking complexity:Managing communication between multiple containers is difficult.
3.Load Balancing:Traffic distribution across multiple containers requires manual setup
4.Fault Tolerance:No built in mechanism to restart failed containers or distribute workloads.


How Kubernetes Solves These Challenges :
1.Automated scaling:Uses the horizontal pod autoscaler to adjust container replicas based on cpu/memory usage.
2.Built-in Networking – Provides service discovery and inter-container communication via kube-proxy and networking policies.
3.Load Balancing – Uses Services to automatically distribute traffic among multiple container instances.


5 Use Cases Where You Should Use Kubernetes 
Microservices Architecture – Ideal for managing multiple interdependent services.
Auto-Scaling Applications – Automatically scales applications based on load.
Multi-Cloud Deployments – Works across AWS, Azure, GCP, and on-premises.
CI/CD Pipelines – Supports automated software deployments.
Big Data & AI Workloads – Handles distributed computing with Spark, TensorFlow, etc.



5 Use Cases Where You Should NOT Use Kubernetes 
Small-Scale Applications – Overhead is too high for simple apps.
Monolithic Applications – Legacy apps that don’t benefit from containerization.
Resource-Constrained Environments – Not suitable for low-power devices or small VMs.
Low-Latency Real-Time Systems – Kubernetes introduces some delay due to orchestration overhead.
Short-Lived Batch Jobs – Simpler container solutions like Docker Swarm may be better.

DAY5:

Kubernetes Architecture Overview:
Kubernetes is a container orchestration platform designed to manage, scale, and automate containerized applications. Its architecture follows a Master-Worker model, where the Control Plane (Master Node) manages the cluster, and Worker Nodes run the actual applications.

1. Control Plane (Master Node) – Brain of Kubernetes
The Control Plane manages the cluster and ensures the desired state of applications. It consists of:

1️⃣ API Server (kube-apiserver)

Acts as the entry point for all Kubernetes operations.
Receives REST API requests and processes them.
2️⃣ Controller Manager (kube-controller-manager)

Monitors and maintains the cluster’s state (e.g., scaling, node health).
Includes controllers like ReplicaSet, Node, and Job controllers.
3️⃣ Scheduler (kube-scheduler)

Assigns Pods to Worker Nodes based on resource availability and constraints.
4️⃣ ETCD (Distributed Key-Value Store)

Stores cluster configuration and state information.
Highly available and consistent database.


 2. Worker Nodes – Runs Applications (Pods & Containers)
Worker Nodes run application workloads and are managed by the Control Plane. Each node has:

1️⃣ Kubelet

Agent running on each worker node.
Ensures containers are running as expected.
2️⃣ Container Runtime (Docker, containerd, CRI-O)

Executes and manages containers.
3️⃣ Kube Proxy (kube-proxy)

Handles networking and communication between services.
Manages network routing for Pods.
4️⃣ Pods

The smallest deployable unit in Kubernetes.
Contains one or more containers.


Other Important Kubernetes Components
✅ Namespaces – Logical separation of resources within a cluster.
✅ ConfigMaps & Secrets – Stores configuration and sensitive data.
✅ Persistent Volumes (PVs) & Persistent Volume Claims (PVCs) – Provides persistent storage.
✅ Ingress Controller – Manages external access to services using domain names.

How Kubernetes Works?
1️⃣ User interacts with the API Server (via kubectl, UI, or API).
2️⃣ Scheduler assigns workloads (Pods) to Worker Nodes.
3️⃣ Kubelet ensures containers run on the assigned nodes.
4️⃣ Kube Proxy manages networking between Pods and external users.
5️⃣ Controller Manager ensures the desired state (self-healing, scaling, etc.).


DAY6:
first learn to install kubectl,and kind cluster .
Below domains and all of its sub-domains are allowed to be referred in the exam

https://kubernetes.io/docs
https://kubernetes.io/blog/
Kubernetes cheat sheet : https://kubernetes.io/docs/reference/kubectl/quick-reference/


DAY7:

Different ways of creating a Kubernetes object
Imperative way ( Through command or API calls)
Declarative way ( By creating manifest files)
Below is the sample pod YAML used in the video:
# This is a sample pod yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    env: demo
    type: frontend
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80 

Task done for DAY7:
1. Create a pod using the imperative command and use nginx as the image:
kubectl run nginx-pod --image=nginx --restart=Never

2. Create the YAML from the nginx pod created in task 1
Update the pod name in the YAML
Use that YAML to create a new pod with the name nginx-new.:

Step 1: Generate YAML from the existing pod:
kubectl get pod nginx-pod -o yaml > nginx-pod.yaml

Step 2: Modify the YAML:
nano nginx-pod.yaml


Step 3: Create a new pod from the updated YAML:
kubectl apply -f nginx-pod.yaml


Step 4: Verify the new pod is running:
kubectl get pods



DAY8:
Replication controller in kubernetes:
it ensures that a specified number of pod replicas are running at all times .it monitor the pods and replaces failed or terminated ones to maintain the desired state

Task :
1. Create a ReplicaSet with 3 Replicas:
Create a file named nginx-replicaset.yaml and add the following content:
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
spec:
  replicas: 3  # Initially, 3 replicas
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx

Apply the yaml file : kubectl apply -f nginx-replicaset.yaml

Deployement:
Step 1: Create a Deployment with 3 Replicas
Create a file named nginx-deployment.yaml and add the following content:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    tier: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: v1
  template:
    metadata:
      labels:
        app: v1
    spec:
      containers:
      - name: nginx
        image: nginx:1.23.0
Apply the YAML file:kubectl apply -f nginx-deployment.yaml

Step 2: List the Deployment and Verify Replicas
kubectl get deployments
kubectl get pods -l app=v1

Step 3: Update the Image to nginx:1.23.4:
kubectl set image deployment/nginx nginx=nginx:1.23.4

Step 4: Assign the Change Cause:
kubectl annotate deployment/nginx kubernetes.io/change-cause="Pick up patch version"

Step 5: Scale the Deployment to 5 Replicas
kubectl scale deployment/nginx --replicas=5


Step 6: Check Deployment Rollout History:
kubectl rollout history deployment/nginx

Step 7: Rollback to Revision 1
kubectl rollout undo deployment/nginx --to-revision=1

Verify the image:
kubectl get pods -o jsonpath='{.items[*].spec.containers[*].image}'



Troubleshooting the issue
Apply the below YAML and fix the issue with it
apiVersion: v1
kind:  Deployment
metadata:
  name: nginx-deploy
  labels:
    env: demo
spec:
  template:
    metadata:
      labels:
        env: demo
      name: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
  replicas: 3
  selector:
    matchLabels:
      env: demo

  Apply the below YAML and fix the issue with it
apiVersion: v1
kind:  Deployment
metadata:
  name: nginx-deploy
  labels:
    env: demo
spec:
  template:
    metadata:
      labels:
        env: demo
      name: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
  replicas: 3
  selector:
    matchLabels:
      env: dev




DAY9:

Pre-requisite for Kind cluster
If you use a Kind cluster, you must perform the port mapping to expose the container port. Use the below config to create a new Kind cluster

kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30001
    hostPort: 30001
- role: worker
- role: worker


command to create new cluster

 kind create cluster --config kind.yaml --name cka-cluster

What is Service in Kubernetes
Different applications communicate with each other within Kubernetes using a service; it is also used to access applications outside the cluster.

There are 4 types of Services:

ClusterIP(For Internal access)
NodePort(To access the application on a particular port)
LoadBalancer(To access the application on a domain name or IP address without using the port number)
External (To use an external DNS for routing)

Sample YAML for ClusterIP
apiVersion: v1
kind: Service
metadata:
  name: cluster-svc
  labels:
    env: demo
spec:
  ports:
  - port: 80
  selector:
    env: demo

Sample YAML for Nodeport
apiVersion: v1
kind: Service
metadata:
  name: nodeport-svc
  labels:
    env: demo
spec:
  type: NodePort
  ports:
  - nodePort: 30001
    port: 80
    targetPort: 80
  selector:
    env: demo

LoadBalancer
Your loadbalancer service will act as nodeport if you are not using any managed cloud Kubernetes such as GKE,AKS,EKS etc. In a managed cloud environment, Kubernetes creates a load balancer within the cloud project, which redirects the traffic to the Kubernetes Loadbalancer service.


Sample YAML for Loadbalancer
apiVersion: v1
kind: Service
metadata:
  name: lb-svc
  labels:
    env: demo
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    env: demo

Sample YAML for external name
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.api.example.com


Task Day9:

1. Create a Service named myapp of type ClusterIP
A ClusterIP service is accessible only within the cluster.
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  selector:
    app: myapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

  Apply the YAML:kubectl apply -f myapp-service.yaml

  Verify:kubectl get svc myapp

  2.Create a Deployment named myapp
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: nginx
        image: nginx:1.23.4-alpine
        ports:
          - containerPort: 80


Apply:kubectl apply -f myapp-deployment.yaml


Verify:
kubectl get deployments myapp
kubectl get pods -l app=myapp


3.Scale the Deployment to 2 replicas
kubectl scale deployment myapp --replicas=2


Verify:
kubectl get deployments myapp
kubectl get pods -l app=myapp


4.Create a temporary Pod using busybox and test the service
kubectl run test-pod --image=busybox -it --rm -- /bin/sh


Inside the busybox shell, run:wget -qO- http://myapp


Exit the pod:exit


5.Run a wget command against the service from outside the cluster
Since the service is ClusterIP, it won’t be accessible externally.
Try this on your local machine:
wget http://<cluster-ip-of-myapp>

This will fail.
To get the ClusterIP:kubectl get svc myapp -o wide


6.Change the Service type to expose it externally

Modify myapp-service.yaml and change the service type to NodePort:
type: NodePort
Apply the change:
kubectl apply -f myapp-service.yaml
Verify:kubectl get svc myapp -o wide


Find the NodePort (e.g., 31234). Access it externally:wget http://<node-ip>:<node-port>



DAY10:
What is a Namespace in Kubernetes
Provides isolation of resources
Avoid accidental deletion/modification
Separated by resource type or environment or domain and so on
Resources can access each other in the same namespace with their first name by the DNS name for other namespaces

Day10 Task:

Step 1: Create Two Namespaces (ns1 and ns2)
kubectl create namespace ns1
kubectl create namespace ns2

Step 2: Deploy Nginx in Both Namespaces
kubectl create deployment deploy-ns1 --image=nginx --replicas=1 -n ns1
kubectl create deployment deploy-ns2 --image=nginx --replicas=1 -n ns2

Step 3: Get the IP Address of Each Pod
kubectl get pods -o wide -n ns1
kubectl get pods -o wide -n ns2

Step 4: Exec into deploy-ns1 Pod and Curl deploy-ns2 Pod
kubectl exec -it <pod-name-in-ns1> -n ns1 -- curl <pod-ip-in-ns2>


Scaling Deployments
Step 5: Scale Deployments to 3 Replicas
kubectl scale deployment deploy-ns1 --replicas=3 -n ns1
kubectl scale deployment deploy-ns2 --replicas=3 -n ns2


Creating Services
Step 6: Expose the Deployments as Services
kubectl expose deployment deploy-ns1 --type=ClusterIP --port=80 --name=svc-ns1 -n ns1
kubectl expose deployment deploy-ns2 --type=ClusterIP --port=80 --name=svc-ns2 -n ns2


Step 7: Get Service IPs and Curl from Pods
kubectl get svc -n ns1
kubectl get svc -n ns2
kubectl exec -it <pod-name-in-ns1> -n ns1 -- curl <svc-ns2-IP>
kubectl exec -it <pod-name-in-ns2> -n ns2 -- curl <svc-ns1-IP>


Using FQDN
Step 8: Try Curling Using Service Name (Fails)
kubectl exec -it <pod-name-in-ns1> -n ns1 -- curl svc-ns2
(This fails because DNS resolution is namespace-specific)

Step 9: Try Curling Using FQDN (Works)
kubectl exec -it <pod-name-in-ns1> -n ns1 -- curl svc-ns2.ns2.svc.cluster.local
kubectl exec -it <pod-name-in-ns2> -n ns2 -- curl svc-ns1.ns1.svc.cluster.local
(FQDN format: <service-name>.<namespace>.svc.cluster.local)


Cleanup
Step 10: Delete Namespaces (Deletes Everything Inside)
kubectl delete namespace ns1
kubectl delete namespace ns2
This will remove all related Deployments, Services, and Pods


DAY11:
Sample YAML used in the demo
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app.kubernetes.io/name: MyApp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    env:
    - name: FIRSTNAME
      value: "Piyush"
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c'] #command to run
    args: ['until nslookup myservice.default.svc.cluster.local; do echo waiting for myservice; sleep 2; done']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c']
    args: ['until nslookup mydb.default.svc.cluster.local; do echo waiting for mydb; sleep 2; done']


    apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app.kubernetes.io/name: MyApp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    env:
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c'] # command to run
    args: # arguments to the command
      - > # multi-line string
        until nslookup myservice.default.svc.cluster.local; do
         echo waiting for myservice;
         sleep 2;
        done;