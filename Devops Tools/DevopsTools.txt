DAY1: Containerization (Docker):

Docker:it is isolated environment with libraries ,application runtime and dependencies.It is lightweight sandbox environment.Everything that is required to run an application like (Build->ship->run)

Understanding Containers V/S Virtual Machines:
Containers:The underlying hardware is physical server,it has only one OS runs on the host machines ,container engine like software like docker or kubernets that manages containers.each container include only the application and its dependencies but share the host OS kernels .
Virtual Machines:It has physical hardware like server storage networking.The OS installed on the physical machines eg window,linux.software like VMware ,virtual box that creates and manages multiple VMs and in guest OS ech vm runs its owns operating system which increases overhead ,and each VM has its own library binaries and dependencies to run application

Containers V/S Virtual machines with the help of a Building and House analogy:
Containers(Building):it is like shared but infrastructure isolated system 
Virtual Machines(House):it is like operating system isolated infrastructure.

A Simple Docker WorkFlow:
It starts with a writing a docker file ,which defines the application environment,including the base OS,dependencies and commands to run the application.next you build docker image using docker build,which packages everything into a reusable format then you run a container from this image using docker run which launches the application in an isolated environment.if needed you can push the image to docker hub for sharing or deployment.finally to manage multiple containers you can use docker compose or kubernets for scalling and automations.

Docker Architecture:
It follows a client server Architecture,the docker client interacts with the docker daemon,which manages container,images networks and storage .the docker daemon runs in the background on the host machine and listens for the command from the client.the docker engine is the core componenet that include daemon,CLI,and API.images are light weight standalone packages contaning application code dependencies and configurations.when an image runs it creates a container an isolated runtime environment.docker also uses registries like docker hub to store share images .this Architecture makes application portable scalable and efficient.

DAY2:How To Dockerize a Project 
Get started
Clone a sample git repository using the below command or use your project for the demo:
cd into the directory
Create an empty file with the name Dockerfile=touch Dockerfile
Using the text editor of your choice, paste the below content ( Details about each of these have already shared in the video):
FROM node:18-alpine
WORKDIR /app
COPY . .
RUN yarn install --production
CMD ["node", "src/index.js"]
EXPOSE 3000

Build the docker image using the application code and Dockerfile=docker build -t day02-todo .

Verify the image has been created and stored locally using the below command:docker images

Create a public repository on hub.docker.com and push the image to remote repo:
docker login
docker tag day02-todo:latest username/new-reponame:tagname
docker images
docker push username/new-reponame:tagname

To pull the image to another environment , you can use below command
docker pull username/new-reponame:tagname

To start the docker container, use below command
docker run -dp 3000:3000 username/new-reponame:tagname

Verify your app. If you have followed the above steps correctly, your app should be listening on localhost:3000
To enter(exec) into the container, use the below command
docker exec -it containername sh
or
docker exec -it containerid sh

To view docker logs
docker logs containername
or
docker logs containerid



DAY3:
Getting started with the demo:Clone the below sample repository, or you can use any web application that you have=git clone https://github.com/piyushsachdeva/todoapp-docker.git
cd into the directory
cd todoapp-docker/
Create an empty file with the name Dockerfile=touch Dockerfile
Using the text editor of your choice, paste the below content: Note: Details about the below Dockerfile have already been shared in the video=FROM node:18-alpine AS installer
WORKDIR /app
COPY package*.json ./
RUN npm install 
COPY . .
RUN npm run build
FROM nginx:latest AS deployer
COPY --from=installer /app/build /usr/share/nginx/html
Build the docker image using the application code and Dockerfile
docker build -t todoapp-docker .
Verify the image has been created and stored locally using the below command:
docker images
Create a public repository on hub.docker.com and push the image to remote repo
docker login
docker tag todoapp-docker:latest username/new-reponame:tagname
docker images
docker push username/new-reponame:tagname
To pull the image to another environment, you can use the below command
docker pull username/new-reponame:tagname
To start the docker container, use the below command
docker run -dp 3000:80 username/new-reponame:tagname
Verify your app. If you have followed the above steps correctly, your app should be listening on localhost:3000
To enter(exec) into the container, use the below command
docker exec -it containername sh
or
docker exec -it containerid sh
To view docker logs
docker logs containername
or
docker logs containerid
To view the content of Docker container
docker inspect
Cleanup the old docker images from local repo using below command:
docker image rm image-id


DAY4:
Challenges of Using Standalone Containers:
1.Manual scaling-you have to manually start,stop,scale containers based on demand.
2.Networking complexity:Managing communication between multiple containers is difficult.
3.Load Balancing:Traffic distribution across multiple containers requires manual setup
4.Fault Tolerance:No built in mechanism to restart failed containers or distribute workloads.


How Kubernetes Solves These Challenges :
1.Automated scaling:Uses the horizontal pod autoscaler to adjust container replicas based on cpu/memory usage.
2.Built-in Networking ‚Äì Provides service discovery and inter-container communication via kube-proxy and networking policies.
3.Load Balancing ‚Äì Uses Services to automatically distribute traffic among multiple container instances.


5 Use Cases Where You Should Use Kubernetes 
Microservices Architecture ‚Äì Ideal for managing multiple interdependent services.
Auto-Scaling Applications ‚Äì Automatically scales applications based on load.
Multi-Cloud Deployments ‚Äì Works across AWS, Azure, GCP, and on-premises.
CI/CD Pipelines ‚Äì Supports automated software deployments.
Big Data & AI Workloads ‚Äì Handles distributed computing with Spark, TensorFlow, etc.



5 Use Cases Where You Should NOT Use Kubernetes 
Small-Scale Applications ‚Äì Overhead is too high for simple apps.
Monolithic Applications ‚Äì Legacy apps that don‚Äôt benefit from containerization.
Resource-Constrained Environments ‚Äì Not suitable for low-power devices or small VMs.
Low-Latency Real-Time Systems ‚Äì Kubernetes introduces some delay due to orchestration overhead.
Short-Lived Batch Jobs ‚Äì Simpler container solutions like Docker Swarm may be better.

DAY5:

Kubernetes Architecture Overview:
Kubernetes is a container orchestration platform designed to manage, scale, and automate containerized applications. Its architecture follows a Master-Worker model, where the Control Plane (Master Node) manages the cluster, and Worker Nodes run the actual applications.

1. Control Plane (Master Node) ‚Äì Brain of Kubernetes
The Control Plane manages the cluster and ensures the desired state of applications. It consists of:

1Ô∏è‚É£ API Server (kube-apiserver)

Acts as the entry point for all Kubernetes operations.
Receives REST API requests and processes them.
2Ô∏è‚É£ Controller Manager (kube-controller-manager)

Monitors and maintains the cluster‚Äôs state (e.g., scaling, node health).
Includes controllers like ReplicaSet, Node, and Job controllers.
3Ô∏è‚É£ Scheduler (kube-scheduler)

Assigns Pods to Worker Nodes based on resource availability and constraints.
4Ô∏è‚É£ ETCD (Distributed Key-Value Store)

Stores cluster configuration and state information.
Highly available and consistent database.


 2. Worker Nodes ‚Äì Runs Applications (Pods & Containers)
Worker Nodes run application workloads and are managed by the Control Plane. Each node has:

1Ô∏è‚É£ Kubelet

Agent running on each worker node.
Ensures containers are running as expected.
2Ô∏è‚É£ Container Runtime (Docker, containerd, CRI-O)

Executes and manages containers.
3Ô∏è‚É£ Kube Proxy (kube-proxy)

Handles networking and communication between services.
Manages network routing for Pods.
4Ô∏è‚É£ Pods

The smallest deployable unit in Kubernetes.
Contains one or more containers.


Other Important Kubernetes Components
‚úÖ Namespaces ‚Äì Logical separation of resources within a cluster.
‚úÖ ConfigMaps & Secrets ‚Äì Stores configuration and sensitive data.
‚úÖ Persistent Volumes (PVs) & Persistent Volume Claims (PVCs) ‚Äì Provides persistent storage.
‚úÖ Ingress Controller ‚Äì Manages external access to services using domain names.

How Kubernetes Works?
1Ô∏è‚É£ User interacts with the API Server (via kubectl, UI, or API).
2Ô∏è‚É£ Scheduler assigns workloads (Pods) to Worker Nodes.
3Ô∏è‚É£ Kubelet ensures containers run on the assigned nodes.
4Ô∏è‚É£ Kube Proxy manages networking between Pods and external users.
5Ô∏è‚É£ Controller Manager ensures the desired state (self-healing, scaling, etc.).


DAY6:
first learn to install kubectl,and kind cluster .
Below domains and all of its sub-domains are allowed to be referred in the exam

https://kubernetes.io/docs
https://kubernetes.io/blog/
Kubernetes cheat sheet : https://kubernetes.io/docs/reference/kubectl/quick-reference/


DAY7:

Different ways of creating a Kubernetes object
Imperative way ( Through command or API calls)
Declarative way ( By creating manifest files)
Below is the sample pod YAML used in the video:
# This is a sample pod yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    env: demo
    type: frontend
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80 

Task done for DAY7:
1. Create a pod using the imperative command and use nginx as the image:
kubectl run nginx-pod --image=nginx --restart=Never

2. Create the YAML from the nginx pod created in task 1
Update the pod name in the YAML
Use that YAML to create a new pod with the name nginx-new.:

Step 1: Generate YAML from the existing pod:
kubectl get pod nginx-pod -o yaml > nginx-pod.yaml

Step 2: Modify the YAML:
nano nginx-pod.yaml


Step 3: Create a new pod from the updated YAML:
kubectl apply -f nginx-pod.yaml


Step 4: Verify the new pod is running:
kubectl get pods



DAY8:
Replication controller in kubernetes:
it ensures that a specified number of pod replicas are running at all times .it monitor the pods and replaces failed or terminated ones to maintain the desired state

Task :
1. Create a ReplicaSet with 3 Replicas:
Create a file named nginx-replicaset.yaml and add the following content:
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs
spec:
  replicas: 3  # Initially, 3 replicas
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx

Apply the yaml file : kubectl apply -f nginx-replicaset.yaml

Deployement:
Step 1: Create a Deployment with 3 Replicas
Create a file named nginx-deployment.yaml and add the following content:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    tier: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: v1
  template:
    metadata:
      labels:
        app: v1
    spec:
      containers:
      - name: nginx
        image: nginx:1.23.0
Apply the YAML file:kubectl apply -f nginx-deployment.yaml

Step 2: List the Deployment and Verify Replicas
kubectl get deployments
kubectl get pods -l app=v1

Step 3: Update the Image to nginx:1.23.4:
kubectl set image deployment/nginx nginx=nginx:1.23.4

Step 4: Assign the Change Cause:
kubectl annotate deployment/nginx kubernetes.io/change-cause="Pick up patch version"

Step 5: Scale the Deployment to 5 Replicas
kubectl scale deployment/nginx --replicas=5


Step 6: Check Deployment Rollout History:
kubectl rollout history deployment/nginx

Step 7: Rollback to Revision 1
kubectl rollout undo deployment/nginx --to-revision=1

Verify the image:
kubectl get pods -o jsonpath='{.items[*].spec.containers[*].image}'



Troubleshooting the issue
Apply the below YAML and fix the issue with it
apiVersion: v1
kind:  Deployment
metadata:
  name: nginx-deploy
  labels:
    env: demo
spec:
  template:
    metadata:
      labels:
        env: demo
      name: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
  replicas: 3
  selector:
    matchLabels:
      env: demo

  Apply the below YAML and fix the issue with it
apiVersion: v1
kind:  Deployment
metadata:
  name: nginx-deploy
  labels:
    env: demo
spec:
  template:
    metadata:
      labels:
        env: demo
      name: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
  replicas: 3
  selector:
    matchLabels:
      env: dev




DAY9:

Pre-requisite for Kind cluster
If you use a Kind cluster, you must perform the port mapping to expose the container port. Use the below config to create a new Kind cluster

kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30001
    hostPort: 30001
- role: worker
- role: worker


command to create new cluster

 kind create cluster --config kind.yaml --name cka-cluster

What is Service in Kubernetes
Different applications communicate with each other within Kubernetes using a service; it is also used to access applications outside the cluster.

There are 4 types of Services:

ClusterIP(For Internal access)
NodePort(To access the application on a particular port)
LoadBalancer(To access the application on a domain name or IP address without using the port number)
External (To use an external DNS for routing)

Sample YAML for ClusterIP
apiVersion: v1
kind: Service
metadata:
  name: cluster-svc
  labels:
    env: demo
spec:
  ports:
  - port: 80
  selector:
    env: demo

Sample YAML for Nodeport
apiVersion: v1
kind: Service
metadata:
  name: nodeport-svc
  labels:
    env: demo
spec:
  type: NodePort
  ports:
  - nodePort: 30001
    port: 80
    targetPort: 80
  selector:
    env: demo

LoadBalancer
Your loadbalancer service will act as nodeport if you are not using any managed cloud Kubernetes such as GKE,AKS,EKS etc. In a managed cloud environment, Kubernetes creates a load balancer within the cloud project, which redirects the traffic to the Kubernetes Loadbalancer service.


Sample YAML for Loadbalancer
apiVersion: v1
kind: Service
metadata:
  name: lb-svc
  labels:
    env: demo
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    env: demo

Sample YAML for external name
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.api.example.com


Task Day9:

1. Create a Service named myapp of type ClusterIP
A ClusterIP service is accessible only within the cluster.
apiVersion: v1
kind: Service
metadata:
  name: myapp
spec:
  selector:
    app: myapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

  Apply the YAML:kubectl apply -f myapp-service.yaml

  Verify:kubectl get svc myapp

  2.Create a Deployment named myapp
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: nginx
        image: nginx:1.23.4-alpine
        ports:
          - containerPort: 80


Apply:kubectl apply -f myapp-deployment.yaml


Verify:
kubectl get deployments myapp
kubectl get pods -l app=myapp


3.Scale the Deployment to 2 replicas
kubectl scale deployment myapp --replicas=2


Verify:
kubectl get deployments myapp
kubectl get pods -l app=myapp


4.Create a temporary Pod using busybox and test the service
kubectl run test-pod --image=busybox -it --rm -- /bin/sh


Inside the busybox shell, run:wget -qO- http://myapp


Exit the pod:exit


5.Run a wget command against the service from outside the cluster
Since the service is ClusterIP, it won‚Äôt be accessible externally.
Try this on your local machine:
wget http://<cluster-ip-of-myapp>

This will fail.
To get the ClusterIP:kubectl get svc myapp -o wide


6.Change the Service type to expose it externally

Modify myapp-service.yaml and change the service type to NodePort:
type: NodePort
Apply the change:
kubectl apply -f myapp-service.yaml
Verify:kubectl get svc myapp -o wide


Find the NodePort (e.g., 31234). Access it externally:wget http://<node-ip>:<node-port>



DAY10:
What is a Namespace in Kubernetes
Provides isolation of resources
Avoid accidental deletion/modification
Separated by resource type or environment or domain and so on
Resources can access each other in the same namespace with their first name by the DNS name for other namespaces

Day10 Task:

Step 1: Create Two Namespaces (ns1 and ns2)
kubectl create namespace ns1
kubectl create namespace ns2

Step 2: Deploy Nginx in Both Namespaces
kubectl create deployment deploy-ns1 --image=nginx --replicas=1 -n ns1
kubectl create deployment deploy-ns2 --image=nginx --replicas=1 -n ns2

Step 3: Get the IP Address of Each Pod
kubectl get pods -o wide -n ns1
kubectl get pods -o wide -n ns2

Step 4: Exec into deploy-ns1 Pod and Curl deploy-ns2 Pod
kubectl exec -it <pod-name-in-ns1> -n ns1 -- curl <pod-ip-in-ns2>


Scaling Deployments
Step 5: Scale Deployments to 3 Replicas
kubectl scale deployment deploy-ns1 --replicas=3 -n ns1
kubectl scale deployment deploy-ns2 --replicas=3 -n ns2


Creating Services
Step 6: Expose the Deployments as Services
kubectl expose deployment deploy-ns1 --type=ClusterIP --port=80 --name=svc-ns1 -n ns1
kubectl expose deployment deploy-ns2 --type=ClusterIP --port=80 --name=svc-ns2 -n ns2


Step 7: Get Service IPs and Curl from Pods
kubectl get svc -n ns1
kubectl get svc -n ns2
kubectl exec -it <pod-name-in-ns1> -n ns1 -- curl <svc-ns2-IP>
kubectl exec -it <pod-name-in-ns2> -n ns2 -- curl <svc-ns1-IP>


Using FQDN
Step 8: Try Curling Using Service Name (Fails)
kubectl exec -it <pod-name-in-ns1> -n ns1 -- curl svc-ns2
(This fails because DNS resolution is namespace-specific)

Step 9: Try Curling Using FQDN (Works)
kubectl exec -it <pod-name-in-ns1> -n ns1 -- curl svc-ns2.ns2.svc.cluster.local
kubectl exec -it <pod-name-in-ns2> -n ns2 -- curl svc-ns1.ns1.svc.cluster.local
(FQDN format: <service-name>.<namespace>.svc.cluster.local)


Cleanup
Step 10: Delete Namespaces (Deletes Everything Inside)
kubectl delete namespace ns1
kubectl delete namespace ns2
This will remove all related Deployments, Services, and Pods


DAY11:
Sample YAML used in the demo
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app.kubernetes.io/name: MyApp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    env:
    - name: FIRSTNAME
      value: "Piyush"
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c'] #command to run
    args: ['until nslookup myservice.default.svc.cluster.local; do echo waiting for myservice; sleep 2; done']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c']
    args: ['until nslookup mydb.default.svc.cluster.local; do echo waiting for mydb; sleep 2; done']


    apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app.kubernetes.io/name: MyApp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    env:
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c'] # command to run
    args: # arguments to the command
      - > # multi-line string
        until nslookup myservice.default.svc.cluster.local; do
         echo waiting for myservice;
         sleep 2;
        done;


Task DAY11:

Create a Multi-Container Pod in Kubernetes:
A multi-container pod in Kubernetes is a pod that runs multiple containers within the same pod. These containers share storage, network, and lifecycle.

1. Create a YAML File for Multi-Container Pod
Create a file named multi-container-pod.yaml and add the following content:
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80
  - name: sidecar-container
    image: busybox
    command: ["sh", "-c", "while true; do echo Sidecar container is running; sleep 5; done"]

2. Apply the YAML File
kubectl apply -f multi-container-pod.yaml


3. Verify the Pod is Running
kubectl get pods

4. Check Logs for Both Containers
kubectl logs multi-container-pod -c nginx-container
kubectl logs multi-container-pod -c sidecar-container

5. Exec Into the Sidecar Container
kubectl exec -it multi-container-pod -c sidecar-container -- sh


DAy12:

What is a daemonset?
A daemon set is another type of Kubernetes object that controls pods. Unlike deployment, the DS automatically deploys 1 pod to each available node. You don't need to update the replica based on demand; the DS takes care of it by spinning X number of pods for X number of nodes.
If you create a ds in a cluster of 5 nodes, then 5 pods will be created.
If you add another node to the cluster, a new pod will be automatically created on the new node.


Examples of daemonset
kube-proxy
calico
weave-net
monitoring agents etc

Sample DS yaml used in the demo
apiVersion: apps/v1
kind:  DaemonSet
metadata:
  name: nginx-ds
  labels:
    env: demo
spec:
  template:
    metadata:
      labels:
        env: demo
      name: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        ports:
        - containerPort: 80
  selector:
    matchLabels:
      env: demo

Task Day12:
1. Create a DaemonSet
A DaemonSet ensures that a pod runs on every node in the cluster. This is useful for logging, monitoring, or running system-level services.

Create a YAML file for DaemonSet (daemonset.yaml):
yaml
Copy
Edit
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: my-daemonset
spec:
  selector:
    matchLabels:
      name: my-daemonset
  template:
    metadata:
      labels:
        name: my-daemonset
    spec:
      containers:
      - name: busybox-container
        image: busybox
        command: ["sh", "-c", "while true; do echo DaemonSet running; sleep 10; done"]
        
Apply the DaemonSet:
kubectl apply -f daemonset.yaml

Verify DaemonSet is running on all nodes:
kubectl get daemonsets
kubectl get pods -o wide
Delete DaemonSet when done:

kubectl delete daemonset my-daemonset


2. Create a CronJob in Kubernetes
A CronJob runs periodic tasks in Kubernetes, similar to a Linux cron job.

Understanding Cron Syntax:
The cron syntax follows this pattern:

Minute Hour Day Month DayOfWeek Command
For example, */5 * * * * means every 5 minutes.

Create a YAML file for CronJob (cronjob.yaml):
apiVersion: batch/v1
kind: CronJob
metadata:
  name: print-message
spec:
  schedule: "*/5 * * * *"  # Runs every 5 minutes
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: busybox-container
            image: busybox
            command: ["sh", "-c", "echo 40daysofkubernetes"]
          restartPolicy: OnFailure


Apply the CronJob:
kubectl apply -f cronjob.yaml

Verify the CronJob:
kubectl get cronjob
kubectl get jobs
kubectl logs -l job-name=print-message


Delete CronJob when done:
kubectl delete cronjob print-message


DAY13:
Labels üè∑Ô∏è
Labels are key-value pairs attached to Kubernetes objects like pods, services, and deployments. They help organize and group resources based on criteria that make sense to you.

Examples of Labels:

environment: production
type: backend
tier: frontend
application: my-app
Selectors üîç
Selectors filter Kubernetes objects based on their labels. This is incredibly useful for querying and managing a subset of objects that meet specific criteria.

Common Usage:

Pods: kubectl get pods --selector app=my-app
Deployments: Used to filter the pods managed by the deployment.
Services: Filter the pods to which the service routes traffic.

Labels vs. Namespaces üåç
Labels: Organize resources within the same or across namespaces.
Namespaces: Provide a way to isolate resources from each other within a cluster.


Annotations üìù

Annotations are similar to labels but attach non-identifying metadata to objects. For example, recording the release version of an application for information purposes or last applied configuration details etc.


Static Pods

Static Pods are special types of pods managed directly by the kubelet on each node rather than through the Kubernetes API server.

Key Characteristics of Static Pods:

Not Managed by the Scheduler: Unlike deployments or replicasets, the Kubernetes scheduler does not manage static pods.
Defined on the Node: Configuration files for static pods are placed directly on the node's file system, and the kubelet watches these files.
Some examples of static pods are: ApiServer, Kube-scheduler, controller-manager, ETCD etc

Managing Static Pods:

SSH into the Node: You will gain access to the node where the static pod is defined.(Mostly the control plane node)
Modify the YAML File: Edit or create the YAML configuration file for the static pod.
Remove the Scheduler YAML: To stop the pod, you must remove or modify the corresponding file directly on the node.
Default location": is usually /etc/kubernetes/manifests/; you can place the pod YAML in the directory, and Kubelet will pick it for scheduling.

Manual Pod Scheduling

Manual scheduling in Kubernetes involves assigning a pod to a specific node rather than letting the scheduler decide.

Key Points:

nodeName Field: Use this field in the pod specification to specify the node where the pod should run.
No Scheduler Involvement: When nodeName is specified, the scheduler bypasses the pod, and it‚Äôs directly assigned to the given node.

Example Configuration:
apiVersion: v1
kind: Pod
metadata:
  name: manual-scheduled-pod
spec:
  nodeName: worker-node-1
  containers:
  - name: nginx
    image: nginx

Note: Kubernetes will place the pod on worker-node-1 with the above configuration.

Task DAY14:

1. Create a Pod and Schedule it Manually Without the Scheduler
By default, the Kubernetes scheduler assigns pods to nodes automatically. However, you can manually schedule a pod by specifying the nodeName field in the pod definition.

Step 1: Create a YAML file for the manually scheduled Pod (manual-pod.yaml)

apiVersion: v1
kind: Pod
metadata:
  name: manual-scheduled-pod
spec:
  nodeName: <your-node-name>  # Replace with the actual node name
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80

 Find your node name using:

 kubectl get nodes

Step 2: Apply the YAML file to create the Pod

kubectl apply -f manual-pod.yaml

Step 3: Verify the Pod is running on the assigned node

kubectl get pods -o wide

2. Restart Control Plane Components by Modifying Static Pod Manifests
Static pods are managed directly by the kubelet on the control plane node. The configuration files for these pods are stored in the static pod manifest directory.

Step 1: SSH into the Control Plane Node
ssh <control-plane-node>

Step 2: Navigate to the Static Pod Manifest Directory
cd /etc/kubernetes/manifests/

Step 3: List the Static Pod Manifests
ls -l

You should see YAML files for control plane components like:

kube-apiserver.yaml

kube-controller-manager.yaml

kube-scheduler.yaml

etcd.yaml

Step 4: Restart Control Plane Components
You can restart a component by modifying its static pod manifest. For example, restart the API Server by touching its file:

touch kube-apiserver.yaml


This triggers the kubelet to restart the component.

Alternatively, delete and recreate a static pod:

mv kube-apiserver.yaml kube-apiserver.yaml.bak
sleep 5
mv kube-apiserver.yaml.bak kube-apiserver.yaml

Step 5: Verify Control Plane Components
Check if the control plane components restarted successfully:

kubectl get pods -n kube-system

Step 6: Exit the Control Plane Node
exit

Step 1: Create the YAML file for the 3 Pods
Create a file named pods.yaml with the following content:


apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    env: test
spec:
  containers:
  - name: nginx-container
    image: nginx

---
apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    env: dev
spec:
  containers:
  - name: nginx-container
    image: nginx

---
apiVersion: v1
kind: Pod
metadata:
  name: pod3
  labels:
    env: prod
spec:
  containers:
  - name: nginx-container
    image: nginx


Step 2: Apply the YAML file to create the Pods

kubectl apply -f pods.yaml


Step 3: Verify the Pods and Their Labels

kubectl get pods --show-labels

Step 4: Filter Pods Based on Labels

Filter Pods with the label env=dev

kubectl get pods -l env=dev
Filter Pods with the label env=prod

kubectl get pods -l env=prod
Filter Pods with either env=dev OR env=prod

kubectl get pods -l 'env in (dev,prod)'


DAY14:

Taints and Tolerations in Kubernetes üößüìú
In this guide, we'll explore taints and tolerations in Kubernetes, essential tools for managing where pods can be scheduled in your cluster.

Taints: Putting Up Fences üö´
Think of taints as "only you are allowed" signs on your Kubernetes nodes. A taint marks a node with a specific characteristic, such as "gpu=true". By default, pods cannot be scheduled on tainted nodes unless they have a special permission called toleration. When a toleration on a pod matches with the taint on the node then only that pod will be scheduled on that node.


Tolerations: Permission Slips for Pods ‚úÖ
Toleration allows a pod to say, "Hey, I can handle that taint. Schedule me anyway!" You define tolerations in the pod specification to let them bypass the taints.


Taints & Tolerations in Action üé¨
Here‚Äôs a breakdown of the commands to manage taints and tolerations:

Tainting a Node:
kubectl taint nodes node1 key=gpu:NoSchedule
This command taints node1 with the key "gpu" and the effect "NoSchedule." Pods without a toleration for this taint won't be scheduled there.

To remove the taint , you add - at the end of the command , like below.

kubectl taint nodes node1 key=gpu:NoSchedule-
Adding toleration to the pod:
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: redis
  name: redis
spec:
  containers:
  - image: redis
    name: redis
  tolerations:
  - key: "gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
Note: This pod specification defines a toleration for the "gpu" taint with the effect "NoSchedule." This allows the pod to be scheduled on tainted nodes.

Labels vs Taints/Tolerations
Labels group nodes based on size, type,env, etc. Unlike taints, labels don't directly affect scheduling but are useful for organizing resources.

Limitations to Remember üöß
Taints and tolerations are powerful tools, but they have limitations. They cannot handle complex expressions like "AND" or "OR." So, what do we use in that case? We use a combination of Taints, tolerance, and Node affinity, which we will discuss in the next day.


DAY14 TASK:

Step 1: Taint the Worker Nodes
You need to taint the worker nodes to restrict scheduling based on GPU availability.

kubectl taint nodes worker01 gpu=true:NoSchedule
kubectl taint nodes worker02 gpu=false:NoSchedule

gpu=true:NoSchedule on worker01 means that only pods with a toleration for gpu=true can be scheduled here.

gpu=false:NoSchedule on worker02 means that only pods with a toleration for gpu=false can be scheduled here.

Step 2: Create a New Pod Without Tolerations
Create a pod using the nginx image.

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
    - name: nginx
      image: nginx

  Apply this configuration:

kubectl apply -f nginx-pod.yaml

Step 6: Verify Pod Scheduling
Check if the pod is running:

kubectl get pods -o wide

Now, the pod should get scheduled on one of the worker nodes since it tolerates the taints.

Step 7: Create a Pod with a Toleration for gpu=true:NoSchedule
Since worker01 has the taint gpu=true:NoSchedule, we need to ensure that the pod has a matching toleration.

Create a YAML file (nginx-pod.yaml) with the following content:

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  tolerations:
    - key: "gpu"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  containers:
    - name: nginx
      image: nginx
This ensures that the pod will only be scheduled on nodes with gpu=true:NoSchedule, which means it should now run on worker01.


Step 8: Apply the Pod Configuration
Run the following command to create the pod:

kubectl apply -f nginx-pod.yaml

Step 9: Verify Pod Scheduling
Check where the pod is running:

kubectl get pods -o wide
If everything is correct, you should see the pod running on worker01.

You can further verify it by describing the pod:

kubectl describe pod nginx-pod
Look for the Node field in the output, and it should show worker01.

step10:Remove the Taint from the Control Plane Node
To delete the taint on the control plane node, first, check the existing taints:

kubectl get nodes -o json | jq '.items[].spec.taints'
Then, remove the taint using:

kubectl taint nodes <control-plane-node-name> node-role.kubernetes.io/control-plane:NoSchedule-
Note: Replace <control-plane-node-name> with the actual name of your control plane node.


Step 11: Create a Pod with the Redis Image
Now, create a new Pod that will be scheduled on the control plane node.

Create a YAML file (redis-pod.yaml):

apiVersion: v1
kind: Pod
metadata:
  name: redis-pod
spec:
  containers:
    - name: redis
      image: redis
Apply the YAML:

kubectl apply -f redis-pod.yaml
Verify the pod is running on the control plane node:

kubectl get pods -o wide


Step 12: Add the Taint Back to the Control Plane Node
Once the pod is scheduled, reapply the taint:

kubectl taint nodes <control-plane-node-name> node-role.kubernetes.io/control-plane:NoSchedule

Verify the taint is added back:

kubectl describe node <control-plane-node-name> | grep Taints


DAY15:
Beyond Node Selectors: Introducing Affinity üöÄ
Node Selectors are great for basic pod placement based on node labels. But what if you need more control over where your pods land? Enter Node Affinity! This feature offers advanced capabilities to fine-tune pod scheduling in your Kubernetes cluster.

Node Affinity: The Powerhouse üî•
Node Affinity lets you define complex rules for where your pods can be scheduled based on node labels. Think of it as creating a wishlist for your pod's ideal home!

Key Features:
Flexibility: Define precise conditions for pod placement.
Control: Decide where your pods can and cannot go with greater granularity.
Adaptability: Allow pods to stay on their nodes even if the labels change after scheduling.

Properties in Node Affinity
requiredDuringSchedulingIgnoredDuringExecution
preferredDuringSchedulingIgnoredDuringExecution 


Required During Scheduling, Ignored During Execution üõ†Ô∏è
This is the strictest type of Node Affinity. Here's how it works:

Specify Node Labels: Define a list of required node labels (e.g., disktype=ssd) in your pod spec.
Exact Match Requirement: The scheduler only places the pod on nodes with those exact labels.
Execution Consistency: Once scheduled, the pod remains on the node even if the label changes.
Example: Targeting SSD Nodes üíæ
Suppose your pod needs high-speed storage. You can create a deployment with a Node Affinity rule that targets nodes labeled disktype=ssd.

YAML Configuration:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis
  name: redis-3
spec:
  containers:
  - image: redis
    name: redis
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd


DAY 15 TASK:
Step 1: Create the Pod YAML
Create a file named nginx-affinity-pod.yaml with the following content:


apiVersion: v1
kind: Pod
metadata:
  name: nginx-affinity-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: disktype
                operator: In
                values:
                  - ssd
  containers:
    - name: nginx
      image: nginx
‚úÖ Step 2: Apply the YAML

kubectl apply -f nginx-affinity-pod.yaml


‚úÖ Step 3: Check Pod Status

kubectl get pods
kubectl describe pod nginx-affinity-pod
You‚Äôll likely see something like this in the Events section:


0/3 nodes are available: 3 node(s) didn't match node affinity/selector terms.
ü§î Why is the Pod not scheduled?
The pod is not scheduled because no node has the label disktype=ssd.

‚úÖ Step 4 (Optional): Fix by labeling a node
Label one of your nodes with disktype=ssd:


kubectl label node <your-node-name> disktype=ssd


Then the pod should get scheduled successfully:

kubectl get pods -o wide

‚úÖ Step 5: Label worker01 Node
Run the following command to label worker01 with disktype=ssd:


kubectl label node worker01 disktype=ssd


You can verify it with:


kubectl get nodes --show-labels

Look for disktype=ssd on worker01.

‚úÖ Step 6: Check Pod Status
Now, check the pod you created earlier:

kubectl get pods -o wide

You should see that the pod is now scheduled on worker01.

To confirm the node it's running on:

kubectl describe pod nginx-affinity-pod | grep -i node
Output should show:

Node:         worker01/...